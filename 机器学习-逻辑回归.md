

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [Linear Regression(线性回归)](#linear-regression线性回归)
- [Logistic Regression(逻辑回归、对数几率回归)(二元逻辑回归)](#logistic-regression逻辑回归-对数几率回归二元逻辑回归)
- [多元逻辑回归](#多元逻辑回归)
- [逻辑回归与线性回归的关系](#逻辑回归与线性回归的关系)
- [References](#references)

<!-- /code_chunk_output -->


#### Linear Regression(线性回归)

拟合符合数据分布的直线，即估计参数w和b

#### Logistic Regression(逻辑回归、对数几率回归)(二元逻辑回归)

Logistic Regression 虽然被称为回归，但其实际上是分类模型，并常用于二分类。Logistic Regression 因其简单、可并行化、可解释强深受工业界喜爱。Logistic 回归的本质是：假设数据服从这个分布，然后使用极大似然估计做参数的估计。

Logistic 分布是一种连续型的概率分布，其分布函数和密度函数分别为：
$$\begin{aligned}
分布函数: F(x) & = P(X \le x) = \frac{1}{1 + e^{-{(x - \mu)}/{\gamma}}} \\
密度函数：f(x) & = F'(x) = \frac{e^{-{(x - \mu)}/{\gamma}}}{\gamma {(1 + e^{-{(x - \mu)}/{\gamma}})^2}}
\end{aligned}$$ Logistic 分布是由其位置和尺度参数定义的连续分布。Logistic 分布的形状与正态分布的形状相似，但是 Logistic 分布的尾部更长，所以我们可以使用 Logistic 分布来建模比正态分布具有更长尾部和更高波峰的数据分布。在深度学习中常用到的 Sigmoid 函数就是 Logistic 的分布函数在$\mu=0, \gamma=1$的特殊形式。

以二分类为例，对于所给数据集假设存在这样的一条直线可以将数据完成线性可分。
![](./images/LR_Decision_Boundary_Example.jpg)

决策边界可以表示为​$w_1x_1 + w_2x_2 + b = 0$，假设某个样本点$h_w(x) = w_1x_1 + w_2x_2 + b > 0$那么可以判断它的类别为 1，这个过程其实是感知机。**Logistic回归还需要加一层，它要找到分类概率$P(Y=1)$与输入向量$x$的直接关系，然后通过比较概率值来判断类别**。

考虑二分类问题，给定数据集
$$D = (x_1, y_1), (x_2, y_2), ..., (x_n, y_n), x_i \in R^n, y_i \in {0, 1}, i=1, 2, ..., N$$ **考虑到$w^Tx+b$取值是连续的，因此它不能拟合离散变量。可以考虑用它来拟合条件概率$P(Y=1|x)$，因为概率的取值也是连续的**。但是对于$w \neq 0$（若等于零向量则没有什么求解的价值）， $w^Tx+b$取值为$R$，不符合概率取值为0到1，因此考虑采用广义线性模型(利用阶跃函数使其转化为0到1的值域)。
$$p(Y=1|x) = \begin{cases}
0, & z \le 0 \\
0.5, & z = 0 \\
1, & z \ge 0
\end{cases}, z = w^Tx+b$$ **以上函数不可微，因此需要使用对数几率函数进行替代**，
$$\begin{aligned}
y & = \frac{1}{1+e^{-(w^Tx+b)}} \\ 
1 & = y(1+e^{-(w^Tx+b)}) \\
1 & = y + ye^{-(w^Tx+b)} \\
1 - y & = ye^{-(w^Tx+b)} \\
ln(\frac{1-y}{y}) & = ln(e^{-(w^Tx+b)}) \\
w^Tx + b & = -ln(\frac{1-y}{y}) = ln(\frac{y}{1-y})
\end{aligned}$$ 我们将$y$视为$x$为正例的概率，则$1-y$为$x$为其反例的概率。两者的比值称为几率(odds)，指该事件发生与不发生的概率比值。若事件发生的概率为$p$，则有
$$\begin{aligned}
w^Tx + b & = ln(\frac{P(Y=1|x)}{1-P(Y=1|x)}) \\
P(Y=1|x) & = \frac{1}{1+e^{-(w^Tx+b)}}
\end{aligned}$$ 也就是说，**输出$Y=1$的对数几率是由输入$x$的线性函数表示的模型，这就是逻辑回归模型**。当$w^Tx+b$的值越接近正无穷，$P(Y=1|x)$概率值也就越接近1。因此逻辑回归的思路是，先拟合决策边界(不局限于线性，还可以是多项式)，再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率。

以上过程中， 使用对数几率函数的意义在于，
- 直接对分类的概率建模，无需实现假设数据分布，从而避免了假设分布不准确带来的问题；
- 不仅可预测出类别，还能得到该预测的概率，这对一些利用概率辅助决策的任务很有用；
- **对数几率函数是任意阶可导的凸函数**，有许多数值优化算法都可以求出最优解。

对于上述结论，在确定了逻辑回归的数学表达式后，剩下就需要确定模型中的参数。此处使用极大似然估计法进行求解。即找到一组参数，使得在这组参数下，我们的数据的似然度（概率）最大。设
$$\begin{aligned}
P(Y=1|x) & = p(x)\\
P(Y=0|x) & = 1 - p(x)
\end{aligned}$$ 又已知二项分布的概率分布表达式为
$$P(X=y) = p(x)^y(1-p(x))^{1-y}$$ 则，似然函数可以表示为
$$L(w) = \prod_{i-1}^{n}{ p(x_i)^{y_i}(1-p(x_i))^{1-y_i} }$$ 为了求解该函数，一般会对该函数取对数，此时
$$\begin{aligned}
lnL(w) & = \sum_{i=1}^{n}{ [y_ilnp(x_i) + (1-y_i)ln(1-p(x_i))] } \\ 
     & = \sum_{i=1}^{n}{ [y_iln{\frac{p(x_i)}{1-p(x_i)}} + ln(1-p(x_i))] } \\
     & = \sum_{i=1}^{n}{ [y_i(w \cdot x_i) + ln(1 + e^{w \cdot x_i})] }
\end{aligned}$$ 此时，该模型的损失函数为
$$J(w) = -\frac{1}{n}lnL(w)$$ 即最大化似然函数和最小化损失函数等价。

求解上述损失函数可以根据梯度下降法或牛顿法进行求解。

#### 多元逻辑回归

相比于二元逻辑回归，多元逻辑回归的重点在于建立自变量$x$与概率$p(x)$的关系。已知逻辑回归表达式
$$y = \frac{1}{ 1 + e^{ -w^Tx } }$$ 令$ w = w_2 - w_1$，
$$\begin{aligned}
y & = \frac{1}{ 1 + e^{ -(w_2 - w_1)^Tx } } \\
& = \frac{1}{ 1 + e^{ -w_2^Tx + w_1^Tx } } \\
& = \frac{1}{ 1 + \frac{e^{w_1^Tx}}{e^{w_2^Tx}}} \\
& = \frac{e^{w_2^Tx}}{e^{w_1^Tx} + e^{w_2^Tx}}
\end{aligned}$$ 即，如果$y>0.5$时，可以被划分为第一类，$y<0.5$时，可以被划分为第0类。此时类别判断函数可以表示为,
$$f(x) = \begin{cases}
0, & \frac{e^{w_2^Tx}}{e^{w_1^Tx} + e^{w_2^Tx}} \le 0.5 \\
1, & \frac{e^{w_2^Tx}}{e^{w_1^Tx} + e^{w_2^Tx}} > 0.5
\end{cases}$$ 可以看到最终的类别预测跟分子成正比。对于多类的情况，第k类的概率正比于$e^{w_k^Tx}$，此时，多分类逻辑回归模型的概率公式为
$$p(Y=k|x) = \frac{e^{w_k^Tx}}{e^{w_1^Tx} + e^{w_2^Tx} + ... + e^{w_n^Tx}}$$ 根据该概率公式即可对多元逻辑回归模型进行参数求解。

#### 逻辑回归与线性回归的关系

**逻辑回归算法是一种广义的线性回归分析方法，其仅在线性回归算法的基础上，套用一个逻辑函数，从而完成对事件发生的概率进行预测。我们在线性回归中可以得到一个预测值，然后将该值通过逻辑函数进行转换，这样就能够将预测值变成概率值，再根据概率值实现分类。**

线性回归的目的是为了拟合出那条符合数据分布的直线，而逻辑回归的目的其实是为了找到一条直线并进行分类预测。

#### References

1. [机器学习 逻辑回归（非常详细）](https://zhuanlan.zhihu.com/p/74874291)